<p>February 2022Summary: I recently sped up GoAWK by switching from a tree-walking interpreter to a bytecode compiler with a virtual machine interpreter.<br>I discuss why it’s faster and how the new interpreter works.<br> |  |  |  |  | A few years ago I wrote , an AWK interpreter written in Go, along with an  describing how it works, how it’s tested, and how I made it faster.GoAWK has been a fun side project, and is  in at least one sizeable open source project, the  stream processor.<br>It even landed me my current job at Canonical.GoAWK previously used a : to execute a code block, it recursively walked the parsed syntax tree.<br>That’s very simple, but not particularly fast.<br>I’ve been wanting to switch to a bytecode compiler and virtual machine interpreter for a while, and I finally got around to it.One of my early programming projects was , a  compiler for DOS.<br>Most Forth compilers, including Third, are simple compilers that use a form of bytecode – called  in the Forth world.<br>So I guess you could say I’ve been interested in virtual machines for 25 years … does that make me a true geek, or just old?It’s not immediately obvious why compiling to virtual instructions and then executing them with a virtual machine is faster than evaluating a syntax tree (“tree-walking”).It’s actually  work up-front: instead of just lexing and parsing into a syntax tree, we now also have a compile step.<br>That said, virtual machine compilers (including GoAWK’s) are usually very simple and non-optimizing, so that step is fast.One reason it’s faster to execute is this: RAM – which stands for Random Access Memory – is not actually  on modern processors.<br>Memory blocks are loaded into fast CPU caches as needed, so when you have to access a new block, it takes about 10x as long as if it’s in the cache.<br>Peter Norvig’s table of  shows how fetching from level 1 cache takes about 0.5 nanosecond, fetching from level 2 cache 14x that long, and fetching from main memory another 14x!Programming with this in mind is called “data-driven design”.<br>I was reminded of how much impact this makes when watching Andrew Kelley’s excellent talk, .<br>Andrew is the creator of the , and his talk describes how he significantly sped up the Zig compiler by applying data-driven design techniques.<br>That talk was what pushed me to think about this for GoAWK.<br>But back to why a virtual machine is faster than tree-walking…A syntax tree is a bunch of node structures that point to other nodes.<br>They’re scattered around in memory, so to evaluate the child nodes you have to follow pointers and jump around in RAM, possibly evicting whatever’s in the cache already.Here’s a diagram of the GoAWK syntax tree for the expression , showing the memory address in hex above each node name:The  is only 48 bytes from the , but the left  is 8KB from that, and then its  is almost 120KB away from that.<br>Cache blocks are typically 64 bytes, so each of those probably requires loading an additional cache block from main memory.<br>Not very cache-friendly.With a virtual machine interpreter, the instructions are in a nice linear array of opcodes (instruction numbers), which will probably be loaded into a cache block all at once.<br>There’s much less jumping around in RAM.<br>Here’s what the GoAWK virtual machine instructions for that same program look like (you can see this “assembly listing” with the new debug flag, ):One of the (relatively few) optimizations GoAWK’s compiler does is shown here: it turns  into a single  instruction when  is an integer constant, rather than the two-instruction sequence  followed by .<br>This means most field lookups only go through the opcode decoding loop once instead of twice.Another reason the virtual machine approach is faster is because there are fewer function calls, and function calls are relatively slow.<br>When evaluating a syntax tree, the  function recursively calls  again to evaluate child nodes.<br>With a virtual machine, that’s all flattened into a single array of opcodes that we loop over – no function calls are needed for dispatching opcodes.GoAWK’s virtual machine uses 32-bit opcodes.<br>Initially I was going to use 8-bit opcodes (where the “byte” in “bytecode” comes from), but 32-bit opcodes were just as fast, and with 32-bit opcodes you avoid the need for variable sized jump offsets: larger AWK scripts may need more than -128 to +127 jump offsets, whereas nobody’s going to need bigger jump offsets than the two billion that 32 bits gives you.<br>64-bit opcodes are unnecessary big, and they were slightly slower as well.Here are the first 10 opcodes (there are 85 total – you can see the full list in ):As you can see in the  assembly listing above, I’m using a stack-based virtual machine.<br>This is simpler to implement, because the compiler doesn’t need to figure out how to allocate registers, it just pushes to and pops from the stack.<br>Stack-based virtual machines may be slightly slower, however – very fast virtual machines like Lua’s are register-based.GoAWK’s compiler is quite simple, with a fairly direct translation from the syntax tree to instructions.<br>I use some specializations for accessing variables of different scopes: for example, fetching a global variable uses the  instruction, fetching a local uses .<br>(As you can see, my instruction naming scheme is extremely creative.)Here’s the assembly listing for a simple program that sums the numbers from 1 through 10:This shows a neat little optimization I copied from Python, whose interpreter added it in Python 3.10 (though I’m sure it’s not a new idea).<br>To compile a  or  loop, it’d be simplest just to do the test at the top, and then use an unconditional  at the bottom of the loop.<br>But that means you’re executing two jump instructions every loop: one at the top and one at the bottom.Instead, we compile the condition twice: once inverted before the loop () and once at the bottom of the loop ().<br>This is slightly more code overall because the condition is repeated, but the loop itself – which is what matters – is one jump instruction shorter.We could almost certainly improve the instruction set further, perhaps adding special instructions for integers or strings when we know the type of the operation ahead of time.<br>That adds complexity, however, and for now I’m going to keep it simple.The one other optimization the GoAWK compiler does is for assignments.<br>Assignments in AWK are expressions, so by default you’d push their value on the stack … only to discard it right away in most cases.<br>And you rarely use the value of an assignment expression.Here’s the assembly for an optimized assignment expression:And here’s what it would look like if we didn’t have that optimization:Below is the , showing the special case used for this optimization.<br>I also include how we compile  statements, to show something quite different.<br>Note how the compiler makes heavy use of Go’s type switch:The virtual machine  function is a single  loop with a big  statement – one  for each opcode.<br>Here’s a snippet showing the instruction fetching and the code to handle a few of the opcodes:As shown above, the virtual machine is implemented as a big  statement with one  per opcode (around 80 cases).<br>Go’s  statement is currently implemented as a binary search through the “case space”.<br>You can think of it as compiling to something like this – for brevity, only a few branches of the tree are shown in full:As you can see, you need to do O(log N) comparisons and jumps to get down to the case you’re interested in.<br>For 80 opcodes, that’s 6 or 7 branches to decode every instruction.As the number of instructions grows, the number of branches grows too (though thankfully that growth is logarithmic, not linear).<br>When I first coded a proof-of-concept virtual machine for GoAWK and just implemented the 7 or 8 instructions I needed for the demo, it gave a huge performance boost, almost 40% faster, because the  only had a few cases.<br>But now that I have all the opcodes in place, it’s “only” 18% faster.It was actually slower than that when I had around 100 opcodes.<br>I removed some specializations that I had thought would speed things up, but with fewer opcodes it meant one less binary search branch and was actually .It’d be great if there were a way to get constant time instruction dispatch, no matter how many instructions we have.<br>Why can’t Go implement  as a table of jump addresses: look up the code address in a table and jump directly to it? It turns out Keith Randall on the Go team is , so we may get it in Go 1.19.I tried Keith’s branch (which only works with  types at the moment) on GoAWK, and it  of a simple microbenchmark by 10%.<br>So I’m definitely looking forward to the Go compiler learning about “jump tables”.Could we do this optimization ourself? What about an array of functions? I tried that, turning the dispatch loop into the following:This only gave me a 1-2% speed increase on GoAWK’s microbenchmarks ().<br>In the end I decided I’d rather stick with the simpler  code and find other ways to improve the speed.<br>And when the Go compiler supports jump tables for , I’ll get a 10% improvement by doing nothing!The  compiler has a non-standard feature called “computed goto”, which allows you to write something like  at the end of each opcode’s code to jump directly to the code for the next opcode.<br>Eli Bendersky has written an , so I won’t dwell on it further here.<br>Most virtual machines written in C use this technique, including CPython and many others.<br>Unfortunately Go doesn’t have computed goto, but again, when  is compiled to a jump table, that will get us half way there.If you’re interested in reading something a bit more academic about how compilers can optimize , read the paper  by Roger Sayle, which was presented at the 2008 GCC Developers’ Summit.Apart from converting from tree-walking to a virtual machine, I’ve recently added a few other optimizations:One problematic  I did was to change GoAWK’s string functions, such as  and , to use Unicode character indexes rather than byte indexes.<br>I knew this was going to change these operations from O(1) to O(N) in the length of the string, but I figured it wouldn’t matter that much because “N is usually small”.That assumption turned out not to be the case: Volodymyr Gubarkov’s  script went from processing a large JSON file in 1 second to over 8 minutes – .<br>This was untenable, so I decided to revert that fix for now, and figure out an O(1) way to address this in future.<br>Arnold Robbins, long-time maintainer of Gawk,  on how Gawk does a lot of work to make string handling efficient.I hope to optimize GoAWK further in the future, and have opened an  to track future performance work.<br>Here are some of the ideas: Here are a few things I’m considering to speed up the virtual machine: is also unnecessarily expensive due to excess allocations and copying when you’re concatenating more than two strings.<br>Currently a multi-concatenation expression like  is compiled to two binary  instructions:It’d be more efficient for the compiler to detect this and output a new  instruction, for example:This is one fewer instruction, but more importantly, it would avoid allocating a temporary string only to have to allocate a new one and copy the bytes over.<br>Concatenating more than two values is quite common in AWK, and this optimization would get better the more values you’re concatenating.<br>would be great to speed up.<br>GoAWK currently uses Go’s  package, but unfortunately it’s .<br>This makes AWK scripts that use regular expressions heavily about half the speed of Gawk and almost a quarter the speed of Mawk.There are two ways to improve this:So how much faster  the virtual machine interpreter? The microbenchmarks – which admittedly are mostly not the kind of scripts you’d write in AWK – got about 18% faster overall.<br>These are elapsed times, so smaller is better (you can see the  or measure them yourself using  followed by  to show these deltas):Increment, decrement, and augmented assignment are so much faster because the virtual machine has dedicated opcodes for them.<br>Variable access has improved considerably too, as have  loops,  statements, binary operators, and many other benchmarks.My more “real world” benchmark suite – most of which I pulled from the  – got 13% faster overall.<br>In this table,  is the new virtual machine interpreter and  is the old tree-walking one.<br>Somewhat unintuitively, the numbers here are the number of times faster it is than the original , so .I definitely like the performance improvements I got.<br>They weren’t quite as much as I was hoping for, but the fact that GoAWK is faster than Gawk for many CPU-bound operations now is pretty cool.<br>It’s still always slower than the performance-fiend Mawk.<br>And for the stuff that AWK is normally used for – string processing and regular expressions – GoAWK still has a lot of room for improvement.To be honest, I’m not entirely sure it was worth the additional 2500 lines of code (for a project that’s only 15,000 lines of code, including tests).<br>If I had an engineering manager overseeing this, I would have expected pushback (“is this going to help us with real-world workloads?”).<br>However, GoAWK was and remains a passion project – I had fun making and sharing this, and that’s enough for me.I’ve merged the compiler and virtual machine and released them in .<br>The Go API and  command should be 100% backwards compatible.<br>It’s been well-tested against my interpreter tests as well as the tests from the original AWK and the relevant ones from Gawk, but file an  if you find something amiss.I hope you’ve enjoyed or learned from this write-up.<br>Please don’t hesitate to contact me with your feedback or ideas.I’d love it if you  – it will motivate me to work on my open source projects and write more good content.<br>Thanks!February 2022Summary: I recently sped up GoAWK by switching from a tree-walking interpreter to a bytecode compiler with a virtual machine interpreter.<br>I discuss why it’s faster and how the new interpreter works.<br> |  |  |  |  | A few years ago I wrote , an AWK interpreter written in Go, along with an  describing how it works, how it’s tested, and how I made it faster.GoAWK has been a fun side project, and is  in at least one sizeable open source project, the  stream processor.<br>It even landed me my current job at Canonical.GoAWK previously used a : to execute a code block, it recursively walked the parsed syntax tree.<br>That’s very simple, but not particularly fast.<br>I’ve been wanting to switch to a bytecode compiler and virtual machine interpreter for a while, and I finally got around to it.One of my early programming projects was , a  compiler for DOS.<br>Most Forth compilers, including Third, are simple compilers that use a form of bytecode – called  in the Forth world.<br>So I guess you could say I’ve been interested in virtual machines for 25 years … does that make me a true geek, or just old?It’s not immediately obvious why compiling to virtual instructions and then executing them with a virtual machine is faster than evaluating a syntax tree (“tree-walking”).It’s actually  work up-front: instead of just lexing and parsing into a syntax tree, we now also have a compile step.<br>That said, virtual machine compilers (including GoAWK’s) are usually very simple and non-optimizing, so that step is fast.One reason it’s faster to execute is this: RAM – which stands for Random Access Memory – is not actually  on modern processors.<br>Memory blocks are loaded into fast CPU caches as needed, so when you have to access a new block, it takes about 10x as long as if it’s in the cache.<br>Peter Norvig’s table of  shows how fetching from level 1 cache takes about 0.5 nanosecond, fetching from level 2 cache 14x that long, and fetching from main memory another 14x!Programming with this in mind is called “data-driven design”.<br>I was reminded of how much impact this makes when watching Andrew Kelley’s excellent talk, .<br>Andrew is the creator of the , and his talk describes how he significantly sped up the Zig compiler by applying data-driven design techniques.<br>That talk was what pushed me to think about this for GoAWK.<br>But back to why a virtual machine is faster than tree-walking…A syntax tree is a bunch of node structures that point to other nodes.<br>They’re scattered around in memory, so to evaluate the child nodes you have to follow pointers and jump around in RAM, possibly evicting whatever’s in the cache already.Here’s a diagram of the GoAWK syntax tree for the expression , showing the memory address in hex above each node name:The  is only 48 bytes from the , but the left  is 8KB from that, and then its  is almost 120KB away from that.<br>Cache blocks are typically 64 bytes, so each of those probably requires loading an additional cache block from main memory.<br>Not very cache-friendly.With a virtual machine interpreter, the instructions are in a nice linear array of opcodes (instruction numbers), which will probably be loaded into a cache block all at once.<br>There’s much less jumping around in RAM.<br>Here’s what the GoAWK virtual machine instructions for that same program look like (you can see this “assembly listing” with the new debug flag, ):One of the (relatively few) optimizations GoAWK’s compiler does is shown here: it turns  into a single  instruction when  is an integer constant, rather than the two-instruction sequence  followed by .<br>This means most field lookups only go through the opcode decoding loop once instead of twice.Another reason the virtual machine approach is faster is because there are fewer function calls, and function calls are relatively slow.<br>When evaluating a syntax tree, the  function recursively calls  again to evaluate child nodes.<br>With a virtual machine, that’s all flattened into a single array of opcodes that we loop over – no function calls are needed for dispatching opcodes.GoAWK’s virtual machine uses 32-bit opcodes.<br>Initially I was going to use 8-bit opcodes (where the “byte” in “bytecode” comes from), but 32-bit opcodes were just as fast, and with 32-bit opcodes you avoid the need for variable sized jump offsets: larger AWK scripts may need more than -128 to +127 jump offsets, whereas nobody’s going to need bigger jump offsets than the two billion that 32 bits gives you.<br>64-bit opcodes are unnecessary big, and they were slightly slower as well.Here are the first 10 opcodes (there are 85 total – you can see the full list in ):As you can see in the  assembly listing above, I’m using a stack-based virtual machine.<br>This is simpler to implement, because the compiler doesn’t need to figure out how to allocate registers, it just pushes to and pops from the stack.<br>Stack-based virtual machines may be slightly slower, however – very fast virtual machines like Lua’s are register-based.GoAWK’s compiler is quite simple, with a fairly direct translation from the syntax tree to instructions.<br>I use some specializations for accessing variables of different scopes: for example, fetching a global variable uses the  instruction, fetching a local uses .<br>(As you can see, my instruction naming scheme is extremely creative.)Here’s the assembly listing for a simple program that sums the numbers from 1 through 10:This shows a neat little optimization I copied from Python, whose interpreter added it in Python 3.10 (though I’m sure it’s not a new idea).<br>To compile a  or  loop, it’d be simplest just to do the test at the top, and then use an unconditional  at the bottom of the loop.<br>But that means you’re executing two jump instructions every loop: one at the top and one at the bottom.Instead, we compile the condition twice: once inverted before the loop () and once at the bottom of the loop ().<br>This is slightly more code overall because the condition is repeated, but the loop itself – which is what matters – is one jump instruction shorter.We could almost certainly improve the instruction set further, perhaps adding special instructions for integers or strings when we know the type of the operation ahead of time.<br>That adds complexity, however, and for now I’m going to keep it simple.The one other optimization the GoAWK compiler does is for assignments.<br>Assignments in AWK are expressions, so by default you’d push their value on the stack … only to discard it right away in most cases.<br>And you rarely use the value of an assignment expression.Here’s the assembly for an optimized assignment expression:And here’s what it would look like if we didn’t have that optimization:Below is the , showing the special case used for this optimization.<br>I also include how we compile  statements, to show something quite different.<br>Note how the compiler makes heavy use of Go’s type switch:The virtual machine  function is a single  loop with a big  statement – one  for each opcode.<br>Here’s a snippet showing the instruction fetching and the code to handle a few of the opcodes:As shown above, the virtual machine is implemented as a big  statement with one  per opcode (around 80 cases).<br>Go’s  statement is currently implemented as a binary search through the “case space”.<br>You can think of it as compiling to something like this – for brevity, only a few branches of the tree are shown in full:As you can see, you need to do O(log N) comparisons and jumps to get down to the case you’re interested in.<br>For 80 opcodes, that’s 6 or 7 branches to decode every instruction.As the number of instructions grows, the number of branches grows too (though thankfully that growth is logarithmic, not linear).<br>When I first coded a proof-of-concept virtual machine for GoAWK and just implemented the 7 or 8 instructions I needed for the demo, it gave a huge performance boost, almost 40% faster, because the  only had a few cases.<br>But now that I have all the opcodes in place, it’s “only” 18% faster.It was actually slower than that when I had around 100 opcodes.<br>I removed some specializations that I had thought would speed things up, but with fewer opcodes it meant one less binary search branch and was actually .It’d be great if there were a way to get constant time instruction dispatch, no matter how many instructions we have.<br>Why can’t Go implement  as a table of jump addresses: look up the code address in a table and jump directly to it? It turns out Keith Randall on the Go team is , so we may get it in Go 1.19.I tried Keith’s branch (which only works with  types at the moment) on GoAWK, and it  of a simple microbenchmark by 10%.<br>So I’m definitely looking forward to the Go compiler learning about “jump tables”.Could we do this optimization ourself? What about an array of functions? I tried that, turning the dispatch loop into the following:This only gave me a 1-2% speed increase on GoAWK’s microbenchmarks ().<br>In the end I decided I’d rather stick with the simpler  code and find other ways to improve the speed.<br>And when the Go compiler supports jump tables for , I’ll get a 10% improvement by doing nothing!The  compiler has a non-standard feature called “computed goto”, which allows you to write something like  at the end of each opcode’s code to jump directly to the code for the next opcode.<br>Eli Bendersky has written an , so I won’t dwell on it further here.<br>Most virtual machines written in C use this technique, including CPython and many others.<br>Unfortunately Go doesn’t have computed goto, but again, when  is compiled to a jump table, that will get us half way there.If you’re interested in reading something a bit more academic about how compilers can optimize , read the paper  by Roger Sayle, which was presented at the 2008 GCC Developers’ Summit.Apart from converting from tree-walking to a virtual machine, I’ve recently added a few other optimizations:One problematic  I did was to change GoAWK’s string functions, such as  and , to use Unicode character indexes rather than byte indexes.<br>I knew this was going to change these operations from O(1) to O(N) in the length of the string, but I figured it wouldn’t matter that much because “N is usually small”.That assumption turned out not to be the case: Volodymyr Gubarkov’s  script went from processing a large JSON file in 1 second to over 8 minutes – .<br>This was untenable, so I decided to revert that fix for now, and figure out an O(1) way to address this in future.<br>Arnold Robbins, long-time maintainer of Gawk,  on how Gawk does a lot of work to make string handling efficient.I hope to optimize GoAWK further in the future, and have opened an  to track future performance work.<br>Here are some of the ideas: Here are a few things I’m considering to speed up the virtual machine: is also unnecessarily expensive due to excess allocations and copying when you’re concatenating more than two strings.<br>Currently a multi-concatenation expression like  is compiled to two binary  instructions:It’d be more efficient for the compiler to detect this and output a new  instruction, for example:This is one fewer instruction, but more importantly, it would avoid allocating a temporary string only to have to allocate a new one and copy the bytes over.<br>Concatenating more than two values is quite common in AWK, and this optimization would get better the more values you’re concatenating.<br>would be great to speed up.<br>GoAWK currently uses Go’s  package, but unfortunately it’s .<br>This makes AWK scripts that use regular expressions heavily about half the speed of Gawk and almost a quarter the speed of Mawk.There are two ways to improve this:So how much faster  the virtual machine interpreter? The microbenchmarks – which admittedly are mostly not the kind of scripts you’d write in AWK – got about 18% faster overall.<br>These are elapsed times, so smaller is better (you can see the  or measure them yourself using  followed by  to show these deltas):Increment, decrement, and augmented assignment are so much faster because the virtual machine has dedicated opcodes for them.<br>Variable access has improved considerably too, as have  loops,  statements, binary operators, and many other benchmarks.My more “real world” benchmark suite – most of which I pulled from the  – got 13% faster overall.<br>In this table,  is the new virtual machine interpreter and  is the old tree-walking one.<br>Somewhat unintuitively, the numbers here are the number of times faster it is than the original , so .I definitely like the performance improvements I got.<br>They weren’t quite as much as I was hoping for, but the fact that GoAWK is faster than Gawk for many CPU-bound operations now is pretty cool.<br>It’s still always slower than the performance-fiend Mawk.<br>And for the stuff that AWK is normally used for – string processing and regular expressions – GoAWK still has a lot of room for improvement.To be honest, I’m not entirely sure it was worth the additional 2500 lines of code (for a project that’s only 15,000 lines of code, including tests).<br>If I had an engineering manager overseeing this, I would have expected pushback (“is this going to help us with real-world workloads?”).<br>However, GoAWK was and remains a passion project – I had fun making and sharing this, and that’s enough for me.I’ve merged the compiler and virtual machine and released them in .<br>The Go API and  command should be 100% backwards compatible.<br>It’s been well-tested against my interpreter tests as well as the tests from the original AWK and the relevant ones from Gawk, but file an  if you find something amiss.I hope you’ve enjoyed or learned from this write-up.<br>Please don’t hesitate to contact me with your feedback or ideas.I’d love it if you  – it will motivate me to work on my open source projects and write more good content.<br>Thanks!</p>
