<p>This is entirely the fault of a friend complaining about trying to move software between different versions of RHEL.<br>I don’t know why I’m doing this, but it seemed like a good idea at the time.<br>Why is this page named what it is? No idea.<br>This was done in Jan 2022.People have Opinions about DLL’s these days.<br>(I know Linux calls them shared objects, but that’s a dumb name, so I’ll call them DLL’s.) DLL’s add a level of complexity to writing and using software, and newer languages like Rust and Go have eschewed them, while Alpine Linux and maybe some other distributions also just don’t bother using them.<br>On the other hand, they exist for a reason, ie sharing compiled code with a common ABI between multiple programs.<br>This has produced a fair amount of discourse the last few years asking interesting questions: Are they necessary? Are they useful? Are they worth the trouble? Can we reinvent the linking process to make the whole system better? These conversations tend to have a lot of Opinion to them and not much actual data, so let’s start collecting data.What data do we collect? Well, I am going to be looking at my everyday Linux system, an x86_64 desktop running Debian Bookworm.<br>This is a quick-and-dirty survey: I want to do this analysis in like 90 minutes or so, and I will never do much with the results beyond going “hmm, that’s neat”.<br>For the data I want it would probably be best to make a small sqlite database, import everything under the sun into it, and then have a set of SQL queries to do the actual analysis, but I don’t particularly like SQL and have to re-learn it every time I use it, so that would take Time.<br>Next best bet would be to write a pipeline in Python or Julia or something and use either CSV or JSON files as intermediate products, but I’m bored of Python and don’t feel like learning Julia right now, so I’m not going to do that either.<br>Hence if I can’t do something with shell scripting, I’m not going to do it.Also note I have a bit of background in data science, but was never terribly good at it, so I’m just doing this for fun.<br>Hence I will write this process down tutorial-style in the hopes of it being interesting to others, or in case someone dares try to reproduce the process.<br>If you’re not interested in the process, just skip to the bottom of each section for the conclusions.<br>The overall question we are trying to answer here is: “How useful are DLL’s?” This data will not answer that question, but may let us start measuring some pieces of it.Easy things first.<br>How often is each DLL in the system actually used?First, we find all executable files:(make sure you don’t have any remote filesystems mounted unless you want to wait, or give the  option to , though that won’t do what you want if you have say  and  on separate drives.How many executable files do we have?Ok.<br>Do we have any duplicates?We don’t, good.Run  on all files in parallel and save the results:If the file is not an ELF executable then it will output  to stderr, not stdout, so we’re gucci, that won’t go into our data file.<br>If the file is an ELF executable that uses no DLL’s, it will output  to stdout, so we can keep track of those too if we want.The  file looks like this:Snip off everything that doesn’t start with a space:Now it looks like this:We only care about the DLL symbol name, so chop off everything after the first non-leading space:Ok, how many lines are in there?So all the DLL’s on the system put together are used 208,000 times.<br>This is a sort of weird measurement, it’s “the sum of the count of the DLL’s used by each executable”.<br>Let’s turn it into something more handy, a frequency count of how many times each DLL is used.So there are 1654 separate DLL’s used on this system.<br>Eyeballing the  file, the top of it looks like this:And the bottom of it looks like this:So we now know how many times each DLL is used on the system.<br>There are some artifacts in there, I’m not sure happened to give us a DLL named  or ; obviously something in our text processing step mangled some names.<br>Taking 30 seconds to eyeball the file doesn’t turn up too many other implausible-looking things, so I don’t care.<br>This is our quick-and-dirty pass after all.Another anomaly worth keeping track of is statically linked executables.<br>Our lame string-processing approach means that the  output of  gets conveniently preserved and tallied up like any other DLL:I didn’t expect many statically linked executables, but I did expect more than that.Anyway, not terribly surprisingly, it appears that there’s a handful of DLL’s that are used by almost everything, a pretty steep decline, and then it tapers off into a long tail.<br>I’d expect it to be something like an exponential dropoff, because .<br>We might dig a bit more into this data set later, but I tend to prefer to do a breadth-first search on these sorts of problems.<br>Touch a lot of little things lightly, then go back and decide what to dig further into.Ok, so we now know how many times each DLL is used.<br>Let’s do the inverse as well, and find out how many DLL’s each executable uses.<br>To do this we have to go allll the way back to .<br>This is why, whenever you do data science, you either script the whole process at once, or you keep every intermediate data product and write down exactly how it’s produced like I’m doing right now.We can’t feed a shell pipeline into xargs, so I can’t just do  or something.<br>Easy way around is to make a shell script that outputs the data we are interested in.<br>So:Then we just runand it bombs out early for some reason.<br>A little digging through the output turns up the error message , so unsurprisingly a single-quote in a file name somewhere is totally hosing our shell script.<br>This is where a sane person would drop bash like the live grenade it is; a few minutes of trying to make the   option cooperate results in it stubbornly saying , which is just so helpful.<br>Eyeballing the data file however, we can find that most instances of single or double quotes are music or data files that are accidentally marked executable, so we can just get rid of them:Sorry , we will not find out how many DLL’s you need linked into your program space when started.Ok but my bash script is now breaking on files that have spaces in their names, which is quite a lot of them.<br>OF COURSE.<br>Most of those files are probably unimportant, but there’s enough of them in odd places that I don’t want to filter them out or spot check them or such.<br>FINE, we’ll do it the STUPID and SLOW way, since I’m sick of Python in my life and I don’t feel like spending the afternoon learning Julia and  might or might not be able to fix the problem but this works dammit:Great, now the file looks like this:The first number is the number of dll’s, and 0 means “not an ELF executable”.<br>So we snip out everything ending with  and remove empty newlines:Great, now we have this:Let’s use awk to just turn it into  so we can sort it sanely:FAK that ALSO fucks up on filenames with spaces in them.<br>This is why you shouldn’t use shell for anything fancy, folks.<br>Okay, let’s change our  script to do the filtering itself, and output the count first because we know that will just be a number with no heckin’ spaces or quotes or any other BS in it:GREAT now it works and we’ve also eliminated an extra step.<br>Fucking hell.<br>Let’s run our slow and terrible brute force loop again:Well that seems to be working, but pretty slowly.<br>While it’s running I might as well try to make  handle the stupid thing to see if I can make it use all my cores.<br>If I can figure out the correct  invocation before the for loop finishes, it wins.Hey, that was easier than expected.<br>However, this is still the part where you go get a cup of tea, maybe a sandwich, and possibly do some push-ups.<br>The first  run processed a similar amount of data in similar ways, but the pipeline and such in  apparently adds enough overhead to make it go from a couple minutes to 15-20, even when using  on a 16-core machine.<br>The bottleneck is never where you expect it.About 300,000 ’s later, as well as some fleeting error messages from  about being unable to parse files correctly that may or may not be interesting to someone someday, we have some results.<br>Unsurprisingly, of course, the  version finishes first and  looks something like this:Some spot-checking looks correct, so we can just sort it and have our DLL usage counts:The bottom of it looks like this:So, the most DLL-hungry program on the system uses 298 DLL’s, and there appears to be another exponential-dropoff-ish frequency distribution to it.<br>Great.<br>Now, hmmm…This produces a file like this:The first column is how many exe files use that number of DLL’s, and the second column is how many DLL’s it uses.<br>So for example there are 160 executables that use exactly 27 DLL’s.<br>Eeeeexcellent.<br>Time for some graphs!First off, the number of DLL’s each exe uses.<br>The X axis is just the exe, this is the data from the  we just produced.<br>As expected it looks vaguely exponential, though there’s a couple humps and bumps in there.Fig 1: DLL’s used per fileNow let’s look at , how many files are using 1 dll, how many are using 2, how many are using 3, etc.<br>The Y axis got really crunched so I made it logarithmic.<br>Because it was late at night, for some reason I used the natural logarithm.<br>So as you can see there are about  executables that use ~5ish DLL’s (about 5000), then a linear-ish-if-you-squint descent to ~3ish executables using ~150 DLL’s, and then a bit of a bumpy long tail after that.Fig 2: DLL frequencyWe’re sort of ascending through this data in reverse order, so the last thing to look at is how many times each individual DLL is used.<br>This is sort of the flip side of Fig 1, and again there was a huge and weird spread of values so I made the Y axis a natural log.<br>Very surprisingly though, it’s still upward-curving… it’s a  distribution.<br>Don’t see those very often! So there’s a bunch of DLL’s that are used 1 time, of course, but the more popular DLL’s get more popular  quickly.Fig 3: Number of times each DLL is usedIt would have been nice to do a frequency graph of Fig 3, the same as I did with Fig 1 and Fig 2, but I forgot so you’ll just have to imagine it.So, we now know how many times each DLL is used on this system, and how many DLL’s each executable uses.<br>Can we do anything actually useful with this data?We can measure how much hard disk space the DLL’s save vs. static linking.<br>This will be an upper bound, since static linking doesn’t necessarily include unused code from a library into an executable, while the DLL doesn’t know what code will and will not be used.This should be pretty easy, we go to our  and just multiply our counts by the size of each DLL… except we didn’t store the full path for each DLL.<br>Okayyyy, we need to go back to our  and pull out the full paths instead of just the file names:This is not actually accurate because our basic  selection doesn’t actually parse the output completely correctly, but will hopefully be somewhere in the right range.<br>So we have to rerun our count again and generate a file of counts for full paths:This gets us a slightly screwy file that looks like this:As you can see, our  call left some artifacts, and it doesn’t quiiiite match our previous  for Various Reasons.<br>You think that there’s only one  on your system? Pshaw, I have .<br>If we were doing shit Right we would have this in a script already and edit it to take out those artifacts; certainly if I ever wanted to reproduce this data set that would be the way to go.<br>Instead I am just gonna edit the file and remove things that aren’t absolute paths.(Random sidenote: notice that  isn’t in this list.<br>That’s because it’s not a real file, but rather a little chunk of code that the Linux kernel puts into every process to make life a little easier for libc or whatever to make certain system calls.<br>See  for more info.)Ok, so NOW we can easily find the size of the files.<br>Like fucking hell I’m gonna try that in bash, and it’s a bright shiny new day, so I finally am going to resort to Python:Mannnnn, what a cruel and terrible language, forcing us to care about crazy things like  and .<br>How dare it not cover up our mistakes for us.Run that sucker and sort the output in descending order:The first line with no file name is our sum total of  * , and it is 155947508820 bytes, or about 145 gigabytes.<br>This seems high to me, but a) this is an upper bound, and b) the numbers don’t lie, right?   And, about 78 GB of that, more than half the total, is saved by the top 10 in this list.<br>That surprises me, though looking back at Fig 3 it makes sense.For reference, the total non- data on this computer’s root filesystem is about 33 GB.<br>And it’s a terabyte hard drive that isn’t even half full, and I have fast internet to do software updates, so all this is mostly irrelevant to me in practice.Potential bug: I’m not sure whether Python’s  follows symlinks, or just gives us the size of the symlink.<br>So this may be all screwy, though I’d expect the resulting sizes to be a lot smaller if it is.<br>Fortunately it’s easy enough to spot check:So  is 1,835,120 and is used 13,134 times, multiply those together and we get 24,102,466,080, the same number of bytes our program reports for .<br>So whew, we are fine.We can measure how much RAM the DLL’s save vs. static linking.<br>This will also be an upper bound, since OS’s don’t necessarily page the entire DLL into memory at once, AFAIK they generally just page in sections of it lazily as they are actually used, and copy-on-write any data that is mutated.<br>This needs a corpus of programs that are actually usually running though, which is more data to collect.<br>On the flip side this is pretty realistic; I don’t know about anyone else, but I have my computers set up to start a fixed set of programs every time they boot, and I generally use those programs every day.<br>On the flip side, I have 16 GB of RAM in my desktop and almost never use more than half of it.So let’s get a count of how many instances of each process I’m running on my machine:However, it is now clear that we now need three different data tables for this analysis: the programs running, the DLL’s used per program, and the bytes used per DLL.<br>Unfortunately this is really getting to the point where the best tool for the job is a relational database, and I promised myself I wouldn’t go that deep.<br>So I’m going to leave this as an exercise to the reader.Draw your own.No? Here’s some to start with:Hey, we actually got a result or two that were surprising! However, I think this sort of data set has a lot of potential for going deeper.<br>Someone should do that, and make a proper database that they can pull queries out of and such.<br>But it probably won’t be me, at least not any time soon.<br>So here some other things I think would be interesting for that hypothetical person to explore:What interesting stuff  we investigate with this kind of approach?So huzzah, you now have some real data for your next Internet Argument, and you know how to (badly) collect more if you didn’t know that already.<br>Go get to work.This is entirely the fault of a friend complaining about trying to move software between different versions of RHEL.<br>I don’t know why I’m doing this, but it seemed like a good idea at the time.<br>Why is this page named what it is? No idea.<br>This was done in Jan 2022.People have Opinions about DLL’s these days.<br>(I know Linux calls them shared objects, but that’s a dumb name, so I’ll call them DLL’s.) DLL’s add a level of complexity to writing and using software, and newer languages like Rust and Go have eschewed them, while Alpine Linux and maybe some other distributions also just don’t bother using them.<br>On the other hand, they exist for a reason, ie sharing compiled code with a common ABI between multiple programs.<br>This has produced a fair amount of discourse the last few years asking interesting questions: Are they necessary? Are they useful? Are they worth the trouble? Can we reinvent the linking process to make the whole system better? These conversations tend to have a lot of Opinion to them and not much actual data, so let’s start collecting data.What data do we collect? Well, I am going to be looking at my everyday Linux system, an x86_64 desktop running Debian Bookworm.<br>This is a quick-and-dirty survey: I want to do this analysis in like 90 minutes or so, and I will never do much with the results beyond going “hmm, that’s neat”.<br>For the data I want it would probably be best to make a small sqlite database, import everything under the sun into it, and then have a set of SQL queries to do the actual analysis, but I don’t particularly like SQL and have to re-learn it every time I use it, so that would take Time.<br>Next best bet would be to write a pipeline in Python or Julia or something and use either CSV or JSON files as intermediate products, but I’m bored of Python and don’t feel like learning Julia right now, so I’m not going to do that either.<br>Hence if I can’t do something with shell scripting, I’m not going to do it.Also note I have a bit of background in data science, but was never terribly good at it, so I’m just doing this for fun.<br>Hence I will write this process down tutorial-style in the hopes of it being interesting to others, or in case someone dares try to reproduce the process.<br>If you’re not interested in the process, just skip to the bottom of each section for the conclusions.<br>The overall question we are trying to answer here is: “How useful are DLL’s?” This data will not answer that question, but may let us start measuring some pieces of it.Easy things first.<br>How often is each DLL in the system actually used?First, we find all executable files:(make sure you don’t have any remote filesystems mounted unless you want to wait, or give the  option to , though that won’t do what you want if you have say  and  on separate drives.How many executable files do we have?Ok.<br>Do we have any duplicates?We don’t, good.Run  on all files in parallel and save the results:If the file is not an ELF executable then it will output  to stderr, not stdout, so we’re gucci, that won’t go into our data file.<br>If the file is an ELF executable that uses no DLL’s, it will output  to stdout, so we can keep track of those too if we want.The  file looks like this:Snip off everything that doesn’t start with a space:Now it looks like this:We only care about the DLL symbol name, so chop off everything after the first non-leading space:Ok, how many lines are in there?So all the DLL’s on the system put together are used 208,000 times.<br>This is a sort of weird measurement, it’s “the sum of the count of the DLL’s used by each executable”.<br>Let’s turn it into something more handy, a frequency count of how many times each DLL is used.So there are 1654 separate DLL’s used on this system.<br>Eyeballing the  file, the top of it looks like this:And the bottom of it looks like this:So we now know how many times each DLL is used on the system.<br>There are some artifacts in there, I’m not sure happened to give us a DLL named  or ; obviously something in our text processing step mangled some names.<br>Taking 30 seconds to eyeball the file doesn’t turn up too many other implausible-looking things, so I don’t care.<br>This is our quick-and-dirty pass after all.Another anomaly worth keeping track of is statically linked executables.<br>Our lame string-processing approach means that the  output of  gets conveniently preserved and tallied up like any other DLL:I didn’t expect many statically linked executables, but I did expect more than that.Anyway, not terribly surprisingly, it appears that there’s a handful of DLL’s that are used by almost everything, a pretty steep decline, and then it tapers off into a long tail.<br>I’d expect it to be something like an exponential dropoff, because .<br>We might dig a bit more into this data set later, but I tend to prefer to do a breadth-first search on these sorts of problems.<br>Touch a lot of little things lightly, then go back and decide what to dig further into.Ok, so we now know how many times each DLL is used.<br>Let’s do the inverse as well, and find out how many DLL’s each executable uses.<br>To do this we have to go allll the way back to .<br>This is why, whenever you do data science, you either script the whole process at once, or you keep every intermediate data product and write down exactly how it’s produced like I’m doing right now.We can’t feed a shell pipeline into xargs, so I can’t just do  or something.<br>Easy way around is to make a shell script that outputs the data we are interested in.<br>So:Then we just runand it bombs out early for some reason.<br>A little digging through the output turns up the error message , so unsurprisingly a single-quote in a file name somewhere is totally hosing our shell script.<br>This is where a sane person would drop bash like the live grenade it is; a few minutes of trying to make the   option cooperate results in it stubbornly saying , which is just so helpful.<br>Eyeballing the data file however, we can find that most instances of single or double quotes are music or data files that are accidentally marked executable, so we can just get rid of them:Sorry , we will not find out how many DLL’s you need linked into your program space when started.Ok but my bash script is now breaking on files that have spaces in their names, which is quite a lot of them.<br>OF COURSE.<br>Most of those files are probably unimportant, but there’s enough of them in odd places that I don’t want to filter them out or spot check them or such.<br>FINE, we’ll do it the STUPID and SLOW way, since I’m sick of Python in my life and I don’t feel like spending the afternoon learning Julia and  might or might not be able to fix the problem but this works dammit:Great, now the file looks like this:The first number is the number of dll’s, and 0 means “not an ELF executable”.<br>So we snip out everything ending with  and remove empty newlines:Great, now we have this:Let’s use awk to just turn it into  so we can sort it sanely:FAK that ALSO fucks up on filenames with spaces in them.<br>This is why you shouldn’t use shell for anything fancy, folks.<br>Okay, let’s change our  script to do the filtering itself, and output the count first because we know that will just be a number with no heckin’ spaces or quotes or any other BS in it:GREAT now it works and we’ve also eliminated an extra step.<br>Fucking hell.<br>Let’s run our slow and terrible brute force loop again:Well that seems to be working, but pretty slowly.<br>While it’s running I might as well try to make  handle the stupid thing to see if I can make it use all my cores.<br>If I can figure out the correct  invocation before the for loop finishes, it wins.Hey, that was easier than expected.<br>However, this is still the part where you go get a cup of tea, maybe a sandwich, and possibly do some push-ups.<br>The first  run processed a similar amount of data in similar ways, but the pipeline and such in  apparently adds enough overhead to make it go from a couple minutes to 15-20, even when using  on a 16-core machine.<br>The bottleneck is never where you expect it.About 300,000 ’s later, as well as some fleeting error messages from  about being unable to parse files correctly that may or may not be interesting to someone someday, we have some results.<br>Unsurprisingly, of course, the  version finishes first and  looks something like this:Some spot-checking looks correct, so we can just sort it and have our DLL usage counts:The bottom of it looks like this:So, the most DLL-hungry program on the system uses 298 DLL’s, and there appears to be another exponential-dropoff-ish frequency distribution to it.<br>Great.<br>Now, hmmm…This produces a file like this:The first column is how many exe files use that number of DLL’s, and the second column is how many DLL’s it uses.<br>So for example there are 160 executables that use exactly 27 DLL’s.<br>Eeeeexcellent.<br>Time for some graphs!First off, the number of DLL’s each exe uses.<br>The X axis is just the exe, this is the data from the  we just produced.<br>As expected it looks vaguely exponential, though there’s a couple humps and bumps in there.Fig 1: DLL’s used per fileNow let’s look at , how many files are using 1 dll, how many are using 2, how many are using 3, etc.<br>The Y axis got really crunched so I made it logarithmic.<br>Because it was late at night, for some reason I used the natural logarithm.<br>So as you can see there are about  executables that use ~5ish DLL’s (about 5000), then a linear-ish-if-you-squint descent to ~3ish executables using ~150 DLL’s, and then a bit of a bumpy long tail after that.Fig 2: DLL frequencyWe’re sort of ascending through this data in reverse order, so the last thing to look at is how many times each individual DLL is used.<br>This is sort of the flip side of Fig 1, and again there was a huge and weird spread of values so I made the Y axis a natural log.<br>Very surprisingly though, it’s still upward-curving… it’s a  distribution.<br>Don’t see those very often! So there’s a bunch of DLL’s that are used 1 time, of course, but the more popular DLL’s get more popular  quickly.Fig 3: Number of times each DLL is usedIt would have been nice to do a frequency graph of Fig 3, the same as I did with Fig 1 and Fig 2, but I forgot so you’ll just have to imagine it.So, we now know how many times each DLL is used on this system, and how many DLL’s each executable uses.<br>Can we do anything actually useful with this data?We can measure how much hard disk space the DLL’s save vs. static linking.<br>This will be an upper bound, since static linking doesn’t necessarily include unused code from a library into an executable, while the DLL doesn’t know what code will and will not be used.This should be pretty easy, we go to our  and just multiply our counts by the size of each DLL… except we didn’t store the full path for each DLL.<br>Okayyyy, we need to go back to our  and pull out the full paths instead of just the file names:This is not actually accurate because our basic  selection doesn’t actually parse the output completely correctly, but will hopefully be somewhere in the right range.<br>So we have to rerun our count again and generate a file of counts for full paths:This gets us a slightly screwy file that looks like this:As you can see, our  call left some artifacts, and it doesn’t quiiiite match our previous  for Various Reasons.<br>You think that there’s only one  on your system? Pshaw, I have .<br>If we were doing shit Right we would have this in a script already and edit it to take out those artifacts; certainly if I ever wanted to reproduce this data set that would be the way to go.<br>Instead I am just gonna edit the file and remove things that aren’t absolute paths.(Random sidenote: notice that  isn’t in this list.<br>That’s because it’s not a real file, but rather a little chunk of code that the Linux kernel puts into every process to make life a little easier for libc or whatever to make certain system calls.<br>See  for more info.)Ok, so NOW we can easily find the size of the files.<br>Like fucking hell I’m gonna try that in bash, and it’s a bright shiny new day, so I finally am going to resort to Python:Mannnnn, what a cruel and terrible language, forcing us to care about crazy things like  and .<br>How dare it not cover up our mistakes for us.Run that sucker and sort the output in descending order:The first line with no file name is our sum total of  * , and it is 155947508820 bytes, or about 145 gigabytes.<br>This seems high to me, but a) this is an upper bound, and b) the numbers don’t lie, right?   And, about 78 GB of that, more than half the total, is saved by the top 10 in this list.<br>That surprises me, though looking back at Fig 3 it makes sense.For reference, the total non- data on this computer’s root filesystem is about 33 GB.<br>And it’s a terabyte hard drive that isn’t even half full, and I have fast internet to do software updates, so all this is mostly irrelevant to me in practice.Potential bug: I’m not sure whether Python’s  follows symlinks, or just gives us the size of the symlink.<br>So this may be all screwy, though I’d expect the resulting sizes to be a lot smaller if it is.<br>Fortunately it’s easy enough to spot check:So  is 1,835,120 and is used 13,134 times, multiply those together and we get 24,102,466,080, the same number of bytes our program reports for .<br>So whew, we are fine.We can measure how much RAM the DLL’s save vs. static linking.<br>This will also be an upper bound, since OS’s don’t necessarily page the entire DLL into memory at once, AFAIK they generally just page in sections of it lazily as they are actually used, and copy-on-write any data that is mutated.<br>This needs a corpus of programs that are actually usually running though, which is more data to collect.<br>On the flip side this is pretty realistic; I don’t know about anyone else, but I have my computers set up to start a fixed set of programs every time they boot, and I generally use those programs every day.<br>On the flip side, I have 16 GB of RAM in my desktop and almost never use more than half of it.So let’s get a count of how many instances of each process I’m running on my machine:However, it is now clear that we now need three different data tables for this analysis: the programs running, the DLL’s used per program, and the bytes used per DLL.<br>Unfortunately this is really getting to the point where the best tool for the job is a relational database, and I promised myself I wouldn’t go that deep.<br>So I’m going to leave this as an exercise to the reader.Draw your own.No? Here’s some to start with:Hey, we actually got a result or two that were surprising! However, I think this sort of data set has a lot of potential for going deeper.<br>Someone should do that, and make a proper database that they can pull queries out of and such.<br>But it probably won’t be me, at least not any time soon.<br>So here some other things I think would be interesting for that hypothetical person to explore:What interesting stuff  we investigate with this kind of approach?So huzzah, you now have some real data for your next Internet Argument, and you know how to (badly) collect more if you didn’t know that already.<br>Go get to work.This is entirely the fault of a friend complaining about trying to move software between different versions of RHEL.<br>I don’t know why I’m doing this, but it seemed like a good idea at the time.<br>Why is this page named what it is? No idea.<br>This was done in Jan 2022.People have Opinions about DLL’s these days.<br>(I know Linux calls them shared objects, but that’s a dumb name, so I’ll call them DLL’s.) DLL’s add a level of complexity to writing and using software, and newer languages like Rust and Go have eschewed them, while Alpine Linux and maybe some other distributions also just don’t bother using them.<br>On the other hand, they exist for a reason, ie sharing compiled code with a common ABI between multiple programs.<br>This has produced a fair amount of discourse the last few years asking interesting questions: Are they necessary? Are they useful? Are they worth the trouble? Can we reinvent the linking process to make the whole system better? These conversations tend to have a lot of Opinion to them and not much actual data, so let’s start collecting data.What data do we collect? Well, I am going to be looking at my everyday Linux system, an x86_64 desktop running Debian Bookworm.<br>This is a quick-and-dirty survey: I want to do this analysis in like 90 minutes or so, and I will never do much with the results beyond going “hmm, that’s neat”.<br>For the data I want it would probably be best to make a small sqlite database, import everything under the sun into it, and then have a set of SQL queries to do the actual analysis, but I don’t particularly like SQL and have to re-learn it every time I use it, so that would take Time.<br>Next best bet would be to write a pipeline in Python or Julia or something and use either CSV or JSON files as intermediate products, but I’m bored of Python and don’t feel like learning Julia right now, so I’m not going to do that either.<br>Hence if I can’t do something with shell scripting, I’m not going to do it.Also note I have a bit of background in data science, but was never terribly good at it, so I’m just doing this for fun.<br>Hence I will write this process down tutorial-style in the hopes of it being interesting to others, or in case someone dares try to reproduce the process.<br>If you’re not interested in the process, just skip to the bottom of each section for the conclusions.<br>The overall question we are trying to answer here is: “How useful are DLL’s?” This data will not answer that question, but may let us start measuring some pieces of it.Easy things first.<br>How often is each DLL in the system actually used?First, we find all executable files:(make sure you don’t have any remote filesystems mounted unless you want to wait, or give the  option to , though that won’t do what you want if you have say  and  on separate drives.How many executable files do we have?Ok.<br>Do we have any duplicates?We don’t, good.Run  on all files in parallel and save the results:If the file is not an ELF executable then it will output  to stderr, not stdout, so we’re gucci, that won’t go into our data file.<br>If the file is an ELF executable that uses no DLL’s, it will output  to stdout, so we can keep track of those too if we want.The  file looks like this:Snip off everything that doesn’t start with a space:Now it looks like this:We only care about the DLL symbol name, so chop off everything after the first non-leading space:Ok, how many lines are in there?So all the DLL’s on the system put together are used 208,000 times.<br>This is a sort of weird measurement, it’s “the sum of the count of the DLL’s used by each executable”.<br>Let’s turn it into something more handy, a frequency count of how many times each DLL is used.So there are 1654 separate DLL’s used on this system.<br>Eyeballing the  file, the top of it looks like this:And the bottom of it looks like this:So we now know how many times each DLL is used on the system.<br>There are some artifacts in there, I’m not sure happened to give us a DLL named  or ; obviously something in our text processing step mangled some names.<br>Taking 30 seconds to eyeball the file doesn’t turn up too many other implausible-looking things, so I don’t care.<br>This is our quick-and-dirty pass after all.Another anomaly worth keeping track of is statically linked executables.<br>Our lame string-processing approach means that the  output of  gets conveniently preserved and tallied up like any other DLL:I didn’t expect many statically linked executables, but I did expect more than that.Anyway, not terribly surprisingly, it appears that there’s a handful of DLL’s that are used by almost everything, a pretty steep decline, and then it tapers off into a long tail.<br>I’d expect it to be something like an exponential dropoff, because .<br>We might dig a bit more into this data set later, but I tend to prefer to do a breadth-first search on these sorts of problems.<br>Touch a lot of little things lightly, then go back and decide what to dig further into.Ok, so we now know how many times each DLL is used.<br>Let’s do the inverse as well, and find out how many DLL’s each executable uses.<br>To do this we have to go allll the way back to .<br>This is why, whenever you do data science, you either script the whole process at once, or you keep every intermediate data product and write down exactly how it’s produced like I’m doing right now.We can’t feed a shell pipeline into xargs, so I can’t just do  or something.<br>Easy way around is to make a shell script that outputs the data we are interested in.<br>So:Then we just runand it bombs out early for some reason.<br>A little digging through the output turns up the error message , so unsurprisingly a single-quote in a file name somewhere is totally hosing our shell script.<br>This is where a sane person would drop bash like the live grenade it is; a few minutes of trying to make the   option cooperate results in it stubbornly saying , which is just so helpful.<br>Eyeballing the data file however, we can find that most instances of single or double quotes are music or data files that are accidentally marked executable, so we can just get rid of them:Sorry , we will not find out how many DLL’s you need linked into your program space when started.Ok but my bash script is now breaking on files that have spaces in their names, which is quite a lot of them.<br>OF COURSE.<br>Most of those files are probably unimportant, but there’s enough of them in odd places that I don’t want to filter them out or spot check them or such.<br>FINE, we’ll do it the STUPID and SLOW way, since I’m sick of Python in my life and I don’t feel like spending the afternoon learning Julia and  might or might not be able to fix the problem but this works dammit:Great, now the file looks like this:The first number is the number of dll’s, and 0 means “not an ELF executable”.<br>So we snip out everything ending with  and remove empty newlines:Great, now we have this:Let’s use awk to just turn it into  so we can sort it sanely:FAK that ALSO fucks up on filenames with spaces in them.<br>This is why you shouldn’t use shell for anything fancy, folks.<br>Okay, let’s change our  script to do the filtering itself, and output the count first because we know that will just be a number with no heckin’ spaces or quotes or any other BS in it:GREAT now it works and we’ve also eliminated an extra step.<br>Fucking hell.<br>Let’s run our slow and terrible brute force loop again:Well that seems to be working, but pretty slowly.<br>While it’s running I might as well try to make  handle the stupid thing to see if I can make it use all my cores.<br>If I can figure out the correct  invocation before the for loop finishes, it wins.Hey, that was easier than expected.<br>However, this is still the part where you go get a cup of tea, maybe a sandwich, and possibly do some push-ups.<br>The first  run processed a similar amount of data in similar ways, but the pipeline and such in  apparently adds enough overhead to make it go from a couple minutes to 15-20, even when using  on a 16-core machine.<br>The bottleneck is never where you expect it.About 300,000 ’s later, as well as some fleeting error messages from  about being unable to parse files correctly that may or may not be interesting to someone someday, we have some results.<br>Unsurprisingly, of course, the  version finishes first and  looks something like this:Some spot-checking looks correct, so we can just sort it and have our DLL usage counts:The bottom of it looks like this:So, the most DLL-hungry program on the system uses 298 DLL’s, and there appears to be another exponential-dropoff-ish frequency distribution to it.<br>Great.<br>Now, hmmm…This produces a file like this:The first column is how many exe files use that number of DLL’s, and the second column is how many DLL’s it uses.<br>So for example there are 160 executables that use exactly 27 DLL’s.<br>Eeeeexcellent.<br>Time for some graphs!First off, the number of DLL’s each exe uses.<br>The X axis is just the exe, this is the data from the  we just produced.<br>As expected it looks vaguely exponential, though there’s a couple humps and bumps in there.Fig 1: DLL’s used per fileNow let’s look at , how many files are using 1 dll, how many are using 2, how many are using 3, etc.<br>The Y axis got really crunched so I made it logarithmic.<br>Because it was late at night, for some reason I used the natural logarithm.<br>So as you can see there are about  executables that use ~5ish DLL’s (about 5000), then a linear-ish-if-you-squint descent to ~3ish executables using ~150 DLL’s, and then a bit of a bumpy long tail after that.Fig 2: DLL frequencyWe’re sort of ascending through this data in reverse order, so the last thing to look at is how many times each individual DLL is used.<br>This is sort of the flip side of Fig 1, and again there was a huge and weird spread of values so I made the Y axis a natural log.<br>Very surprisingly though, it’s still upward-curving… it’s a  distribution.<br>Don’t see those very often! So there’s a bunch of DLL’s that are used 1 time, of course, but the more popular DLL’s get more popular  quickly.Fig 3: Number of times each DLL is usedIt would have been nice to do a frequency graph of Fig 3, the same as I did with Fig 1 and Fig 2, but I forgot so you’ll just have to imagine it.So, we now know how many times each DLL is used on this system, and how many DLL’s each executable uses.<br>Can we do anything actually useful with this data?We can measure how much hard disk space the DLL’s save vs. static linking.<br>This will be an upper bound, since static linking doesn’t necessarily include unused code from a library into an executable, while the DLL doesn’t know what code will and will not be used.This should be pretty easy, we go to our  and just multiply our counts by the size of each DLL… except we didn’t store the full path for each DLL.<br>Okayyyy, we need to go back to our  and pull out the full paths instead of just the file names:This is not actually accurate because our basic  selection doesn’t actually parse the output completely correctly, but will hopefully be somewhere in the right range.<br>So we have to rerun our count again and generate a file of counts for full paths:This gets us a slightly screwy file that looks like this:As you can see, our  call left some artifacts, and it doesn’t quiiiite match our previous  for Various Reasons.<br>You think that there’s only one  on your system? Pshaw, I have .<br>If we were doing shit Right we would have this in a script already and edit it to take out those artifacts; certainly if I ever wanted to reproduce this data set that would be the way to go.<br>Instead I am just gonna edit the file and remove things that aren’t absolute paths.(Random sidenote: notice that  isn’t in this list.<br>That’s because it’s not a real file, but rather a little chunk of code that the Linux kernel puts into every process to make life a little easier for libc or whatever to make certain system calls.<br>See  for more info.)Ok, so NOW we can easily find the size of the files.<br>Like fucking hell I’m gonna try that in bash, and it’s a bright shiny new day, so I finally am going to resort to Python:Mannnnn, what a cruel and terrible language, forcing us to care about crazy things like  and .<br>How dare it not cover up our mistakes for us.Run that sucker and sort the output in descending order:The first line with no file name is our sum total of  * , and it is 155947508820 bytes, or about 145 gigabytes.<br>This seems high to me, but a) this is an upper bound, and b) the numbers don’t lie, right?   And, about 78 GB of that, more than half the total, is saved by the top 10 in this list.<br>That surprises me, though looking back at Fig 3 it makes sense.For reference, the total non- data on this computer’s root filesystem is about 33 GB.<br>And it’s a terabyte hard drive that isn’t even half full, and I have fast internet to do software updates, so all this is mostly irrelevant to me in practice.Potential bug: I’m not sure whether Python’s  follows symlinks, or just gives us the size of the symlink.<br>So this may be all screwy, though I’d expect the resulting sizes to be a lot smaller if it is.<br>Fortunately it’s easy enough to spot check:So  is 1,835,120 and is used 13,134 times, multiply those together and we get 24,102,466,080, the same number of bytes our program reports for .<br>So whew, we are fine.We can measure how much RAM the DLL’s save vs. static linking.<br>This will also be an upper bound, since OS’s don’t necessarily page the entire DLL into memory at once, AFAIK they generally just page in sections of it lazily as they are actually used, and copy-on-write any data that is mutated.<br>This needs a corpus of programs that are actually usually running though, which is more data to collect.<br>On the flip side this is pretty realistic; I don’t know about anyone else, but I have my computers set up to start a fixed set of programs every time they boot, and I generally use those programs every day.<br>On the flip side, I have 16 GB of RAM in my desktop and almost never use more than half of it.So let’s get a count of how many instances of each process I’m running on my machine:However, it is now clear that we now need three different data tables for this analysis: the programs running, the DLL’s used per program, and the bytes used per DLL.<br>Unfortunately this is really getting to the point where the best tool for the job is a relational database, and I promised myself I wouldn’t go that deep.<br>So I’m going to leave this as an exercise to the reader.Draw your own.No? Here’s some to start with:Hey, we actually got a result or two that were surprising! However, I think this sort of data set has a lot of potential for going deeper.<br>Someone should do that, and make a proper database that they can pull queries out of and such.<br>But it probably won’t be me, at least not any time soon.<br>So here some other things I think would be interesting for that hypothetical person to explore:What interesting stuff  we investigate with this kind of approach?So huzzah, you now have some real data for your next Internet Argument, and you know how to (badly) collect more if you didn’t know that already.<br>Go get to work.This is entirely the fault of a friend complaining about trying to move software between different versions of RHEL.<br>I don’t know why I’m doing this, but it seemed like a good idea at the time.<br>Why is this page named what it is? No idea.<br>This was done in Jan 2022.People have Opinions about DLL’s these days.<br>(I know Linux calls them shared objects, but that’s a dumb name, so I’ll call them DLL’s.) DLL’s add a level of complexity to writing and using software, and newer languages like Rust and Go have eschewed them, while Alpine Linux and maybe some other distributions also just don’t bother using them.<br>On the other hand, they exist for a reason, ie sharing compiled code with a common ABI between multiple programs.<br>This has produced a fair amount of discourse the last few years asking interesting questions: Are they necessary? Are they useful? Are they worth the trouble? Can we reinvent the linking process to make the whole system better? These conversations tend to have a lot of Opinion to them and not much actual data, so let’s start collecting data.What data do we collect? Well, I am going to be looking at my everyday Linux system, an x86_64 desktop running Debian Bookworm.<br>This is a quick-and-dirty survey: I want to do this analysis in like 90 minutes or so, and I will never do much with the results beyond going “hmm, that’s neat”.<br>For the data I want it would probably be best to make a small sqlite database, import everything under the sun into it, and then have a set of SQL queries to do the actual analysis, but I don’t particularly like SQL and have to re-learn it every time I use it, so that would take Time.<br>Next best bet would be to write a pipeline in Python or Julia or something and use either CSV or JSON files as intermediate products, but I’m bored of Python and don’t feel like learning Julia right now, so I’m not going to do that either.<br>Hence if I can’t do something with shell scripting, I’m not going to do it.Also note I have a bit of background in data science, but was never terribly good at it, so I’m just doing this for fun.<br>Hence I will write this process down tutorial-style in the hopes of it being interesting to others, or in case someone dares try to reproduce the process.<br>If you’re not interested in the process, just skip to the bottom of each section for the conclusions.<br>The overall question we are trying to answer here is: “How useful are DLL’s?” This data will not answer that question, but may let us start measuring some pieces of it.Easy things first.<br>How often is each DLL in the system actually used?First, we find all executable files:(make sure you don’t have any remote filesystems mounted unless you want to wait, or give the  option to , though that won’t do what you want if you have say  and  on separate drives.How many executable files do we have?Ok.<br>Do we have any duplicates?We don’t, good.Run  on all files in parallel and save the results:If the file is not an ELF executable then it will output  to stderr, not stdout, so we’re gucci, that won’t go into our data file.<br>If the file is an ELF executable that uses no DLL’s, it will output  to stdout, so we can keep track of those too if we want.The  file looks like this:Snip off everything that doesn’t start with a space:Now it looks like this:We only care about the DLL symbol name, so chop off everything after the first non-leading space:Ok, how many lines are in there?So all the DLL’s on the system put together are used 208,000 times.<br>This is a sort of weird measurement, it’s “the sum of the count of the DLL’s used by each executable”.<br>Let’s turn it into something more handy, a frequency count of how many times each DLL is used.So there are 1654 separate DLL’s used on this system.<br>Eyeballing the  file, the top of it looks like this:And the bottom of it looks like this:So we now know how many times each DLL is used on the system.<br>There are some artifacts in there, I’m not sure happened to give us a DLL named  or ; obviously something in our text processing step mangled some names.<br>Taking 30 seconds to eyeball the file doesn’t turn up too many other implausible-looking things, so I don’t care.<br>This is our quick-and-dirty pass after all.Another anomaly worth keeping track of is statically linked executables.<br>Our lame string-processing approach means that the  output of  gets conveniently preserved and tallied up like any other DLL:I didn’t expect many statically linked executables, but I did expect more than that.Anyway, not terribly surprisingly, it appears that there’s a handful of DLL’s that are used by almost everything, a pretty steep decline, and then it tapers off into a long tail.<br>I’d expect it to be something like an exponential dropoff, because .<br>We might dig a bit more into this data set later, but I tend to prefer to do a breadth-first search on these sorts of problems.<br>Touch a lot of little things lightly, then go back and decide what to dig further into.Ok, so we now know how many times each DLL is used.<br>Let’s do the inverse as well, and find out how many DLL’s each executable uses.<br>To do this we have to go allll the way back to .<br>This is why, whenever you do data science, you either script the whole process at once, or you keep every intermediate data product and write down exactly how it’s produced like I’m doing right now.We can’t feed a shell pipeline into xargs, so I can’t just do  or something.<br>Easy way around is to make a shell script that outputs the data we are interested in.<br>So:Then we just runand it bombs out early for some reason.<br>A little digging through the output turns up the error message , so unsurprisingly a single-quote in a file name somewhere is totally hosing our shell script.<br>This is where a sane person would drop bash like the live grenade it is; a few minutes of trying to make the   option cooperate results in it stubbornly saying , which is just so helpful.<br>Eyeballing the data file however, we can find that most instances of single or double quotes are music or data files that are accidentally marked executable, so we can just get rid of them:Sorry , we will not find out how many DLL’s you need linked into your program space when started.Ok but my bash script is now breaking on files that have spaces in their names, which is quite a lot of them.<br>OF COURSE.<br>Most of those files are probably unimportant, but there’s enough of them in odd places that I don’t want to filter them out or spot check them or such.<br>FINE, we’ll do it the STUPID and SLOW way, since I’m sick of Python in my life and I don’t feel like spending the afternoon learning Julia and  might or might not be able to fix the problem but this works dammit:Great, now the file looks like this:The first number is the number of dll’s, and 0 means “not an ELF executable”.<br>So we snip out everything ending with  and remove empty newlines:Great, now we have this:Let’s use awk to just turn it into  so we can sort it sanely:FAK that ALSO fucks up on filenames with spaces in them.<br>This is why you shouldn’t use shell for anything fancy, folks.<br>Okay, let’s change our  script to do the filtering itself, and output the count first because we know that will just be a number with no heckin’ spaces or quotes or any other BS in it:GREAT now it works and we’ve also eliminated an extra step.<br>Fucking hell.<br>Let’s run our slow and terrible brute force loop again:Well that seems to be working, but pretty slowly.<br>While it’s running I might as well try to make  handle the stupid thing to see if I can make it use all my cores.<br>If I can figure out the correct  invocation before the for loop finishes, it wins.Hey, that was easier than expected.<br>However, this is still the part where you go get a cup of tea, maybe a sandwich, and possibly do some push-ups.<br>The first  run processed a similar amount of data in similar ways, but the pipeline and such in  apparently adds enough overhead to make it go from a couple minutes to 15-20, even when using  on a 16-core machine.<br>The bottleneck is never where you expect it.About 300,000 ’s later, as well as some fleeting error messages from  about being unable to parse files correctly that may or may not be interesting to someone someday, we have some results.<br>Unsurprisingly, of course, the  version finishes first and  looks something like this:Some spot-checking looks correct, so we can just sort it and have our DLL usage counts:The bottom of it looks like this:So, the most DLL-hungry program on the system uses 298 DLL’s, and there appears to be another exponential-dropoff-ish frequency distribution to it.<br>Great.<br>Now, hmmm…This produces a file like this:The first column is how many exe files use that number of DLL’s, and the second column is how many DLL’s it uses.<br>So for example there are 160 executables that use exactly 27 DLL’s.<br>Eeeeexcellent.<br>Time for some graphs!First off, the number of DLL’s each exe uses.<br>The X axis is just the exe, this is the data from the  we just produced.<br>As expected it looks vaguely exponential, though there’s a couple humps and bumps in there.Fig 1: DLL’s used per fileNow let’s look at , how many files are using 1 dll, how many are using 2, how many are using 3, etc.<br>The Y axis got really crunched so I made it logarithmic.<br>Because it was late at night, for some reason I used the natural logarithm.<br>So as you can see there are about  executables that use ~5ish DLL’s (about 5000), then a linear-ish-if-you-squint descent to ~3ish executables using ~150 DLL’s, and then a bit of a bumpy long tail after that.Fig 2: DLL frequencyWe’re sort of ascending through this data in reverse order, so the last thing to look at is how many times each individual DLL is used.<br>This is sort of the flip side of Fig 1, and again there was a huge and weird spread of values so I made the Y axis a natural log.<br>Very surprisingly though, it’s still upward-curving… it’s a  distribution.<br>Don’t see those very often! So there’s a bunch of DLL’s that are used 1 time, of course, but the more popular DLL’s get more popular  quickly.Fig 3: Number of times each DLL is usedIt would have been nice to do a frequency graph of Fig 3, the same as I did with Fig 1 and Fig 2, but I forgot so you’ll just have to imagine it.So, we now know how many times each DLL is used on this system, and how many DLL’s each executable uses.<br>Can we do anything actually useful with this data?We can measure how much hard disk space the DLL’s save vs. static linking.<br>This will be an upper bound, since static linking doesn’t necessarily include unused code from a library into an executable, while the DLL doesn’t know what code will and will not be used.This should be pretty easy, we go to our  and just multiply our counts by the size of each DLL… except we didn’t store the full path for each DLL.<br>Okayyyy, we need to go back to our  and pull out the full paths instead of just the file names:This is not actually accurate because our basic  selection doesn’t actually parse the output completely correctly, but will hopefully be somewhere in the right range.<br>So we have to rerun our count again and generate a file of counts for full paths:This gets us a slightly screwy file that looks like this:As you can see, our  call left some artifacts, and it doesn’t quiiiite match our previous  for Various Reasons.<br>You think that there’s only one  on your system? Pshaw, I have .<br>If we were doing shit Right we would have this in a script already and edit it to take out those artifacts; certainly if I ever wanted to reproduce this data set that would be the way to go.<br>Instead I am just gonna edit the file and remove things that aren’t absolute paths.(Random sidenote: notice that  isn’t in this list.<br>That’s because it’s not a real file, but rather a little chunk of code that the Linux kernel puts into every process to make life a little easier for libc or whatever to make certain system calls.<br>See  for more info.)Ok, so NOW we can easily find the size of the files.<br>Like fucking hell I’m gonna try that in bash, and it’s a bright shiny new day, so I finally am going to resort to Python:Mannnnn, what a cruel and terrible language, forcing us to care about crazy things like  and .<br>How dare it not cover up our mistakes for us.Run that sucker and sort the output in descending order:The first line with no file name is our sum total of  * , and it is 155947508820 bytes, or about 145 gigabytes.<br>This seems high to me, but a) this is an upper bound, and b) the numbers don’t lie, right?   And, about 78 GB of that, more than half the total, is saved by the top 10 in this list.<br>That surprises me, though looking back at Fig 3 it makes sense.For reference, the total non- data on this computer’s root filesystem is about 33 GB.<br>And it’s a terabyte hard drive that isn’t even half full, and I have fast internet to do software updates, so all this is mostly irrelevant to me in practice.Potential bug: I’m not sure whether Python’s  follows symlinks, or just gives us the size of the symlink.<br>So this may be all screwy, though I’d expect the resulting sizes to be a lot smaller if it is.<br>Fortunately it’s easy enough to spot check:So  is 1,835,120 and is used 13,134 times, multiply those together and we get 24,102,466,080, the same number of bytes our program reports for .<br>So whew, we are fine.We can measure how much RAM the DLL’s save vs. static linking.<br>This will also be an upper bound, since OS’s don’t necessarily page the entire DLL into memory at once, AFAIK they generally just page in sections of it lazily as they are actually used, and copy-on-write any data that is mutated.<br>This needs a corpus of programs that are actually usually running though, which is more data to collect.<br>On the flip side this is pretty realistic; I don’t know about anyone else, but I have my computers set up to start a fixed set of programs every time they boot, and I generally use those programs every day.<br>On the flip side, I have 16 GB of RAM in my desktop and almost never use more than half of it.So let’s get a count of how many instances of each process I’m running on my machine:However, it is now clear that we now need three different data tables for this analysis: the programs running, the DLL’s used per program, and the bytes used per DLL.<br>Unfortunately this is really getting to the point where the best tool for the job is a relational database, and I promised myself I wouldn’t go that deep.<br>So I’m going to leave this as an exercise to the reader.Draw your own.No? Here’s some to start with:Hey, we actually got a result or two that were surprising! However, I think this sort of data set has a lot of potential for going deeper.<br>Someone should do that, and make a proper database that they can pull queries out of and such.<br>But it probably won’t be me, at least not any time soon.<br>So here some other things I think would be interesting for that hypothetical person to explore:What interesting stuff  we investigate with this kind of approach?So huzzah, you now have some real data for your next Internet Argument, and you know how to (badly) collect more if you didn’t know that already.<br>Go get to work.</p>
